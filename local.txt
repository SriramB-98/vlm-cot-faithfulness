cml22.umiacs.umd.edu
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]
Context bias:no_context
	 Test bias:no_context
  0%|          | 0/25 [00:00<?, ?it/s]/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
  4%|▍         | 1/25 [00:01<00:32,  1.36s/it]  8%|▊         | 2/25 [00:01<00:20,  1.13it/s] 12%|█▏        | 3/25 [00:02<00:15,  1.41it/s] 16%|█▌        | 4/25 [00:02<00:13,  1.59it/s] 20%|██        | 5/25 [00:03<00:11,  1.71it/s] 24%|██▍       | 6/25 [00:03<00:10,  1.80it/s] 28%|██▊       | 7/25 [00:04<00:09,  1.86it/s] 32%|███▏      | 8/25 [00:04<00:08,  1.90it/s] 36%|███▌      | 9/25 [00:05<00:08,  1.93it/s] 40%|████      | 10/25 [00:05<00:07,  1.89it/s] 44%|████▍     | 11/25 [00:06<00:07,  1.92it/s] 48%|████▊     | 12/25 [00:07<00:06,  1.89it/s] 52%|█████▏    | 13/25 [00:07<00:06,  1.91it/s] 56%|█████▌    | 14/25 [00:08<00:05,  1.94it/s] 60%|██████    | 15/25 [00:08<00:05,  1.95it/s] 64%|██████▍   | 16/25 [00:09<00:04,  1.96it/s] 68%|██████▊   | 17/25 [00:09<00:04,  1.97it/s] 72%|███████▏  | 18/25 [00:10<00:03,  1.92it/s] 76%|███████▌  | 19/25 [00:10<00:03,  1.94it/s] 80%|████████  | 20/25 [00:11<00:02,  1.90it/s] 84%|████████▍ | 21/25 [00:11<00:02,  1.92it/s] 88%|████████▊ | 22/25 [00:12<00:01,  1.89it/s] 92%|█████████▏| 23/25 [00:12<00:01,  1.91it/s] 96%|█████████▌| 24/25 [00:13<00:00,  1.88it/s] 96%|█████████▌| 24/25 [00:13<00:00,  1.74it/s]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) pillow\n(B) desk'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the desk (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) desk\n(B) chair'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the door (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) door\n(B) lamp'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the blinds (highlighted by a blue box)?\n(A) lamp\n(B) blinds'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the television (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) television\n(B) sofa'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the books (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) books\n(B) bookcase'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the sink (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) sink\n(B) pillow'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the table (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) table\n(B) television'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the door (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) door\n(B) pillow'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the refrigerator (highlighted by a blue box)?\n(A) lamp\n(B) refrigerator'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the desk (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) desk\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) refrigerator\n(B) door'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the chair (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) chair\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the books (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) books\n(B) lamp'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the table (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) table\n(B) door'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the television (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) television\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the shelves (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) shelves\n(B) television'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1073]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) bookcase\n(B) books'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
Messages [{'role': 'user', 'content': [{'type': 'text', 'text': 'Question:'}, {'type': 'image'}, {'type': 'text', 'text': 'Which object is closer to the camera taking this photo, the chair (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) chair\n(B) bookcase'}, {'type': 'text', 'text': 'End your answer with either (A) or (B).'}]}]
In Qwen2_5_VLImageProcessor.preprocess pixel_values.shape (3996, 1176)
Input token length [1075]
