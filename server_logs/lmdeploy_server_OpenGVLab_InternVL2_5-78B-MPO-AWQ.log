Fetching 43 files:   0%|          | 0/43 [00:00<?, ?it/s]Fetching 43 files: 100%|██████████| 43/43 [00:00<00:00, 1476.60it/s]
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/accelerate/utils/modeling.py:1589: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=torch.device("cpu"))
Convert to turbomind format:   0%|          | 0/80 [00:00<?, ?it/s]/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/lmdeploy/turbomind/deploy/loader.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tmp = torch.load(shard, map_location='cpu')
Convert to turbomind format:   1%|▏         | 1/80 [00:24<31:46, 24.13s/it]Convert to turbomind format:   2%|▎         | 2/80 [00:26<15:00, 11.54s/it]Convert to turbomind format:   4%|▍         | 3/80 [00:26<08:07,  6.33s/it]Convert to turbomind format:   5%|▌         | 4/80 [00:27<04:54,  3.88s/it]Convert to turbomind format:   6%|▋         | 5/80 [00:27<03:09,  2.53s/it]Convert to turbomind format:   8%|▊         | 6/80 [00:30<03:21,  2.73s/it]Convert to turbomind format:   9%|▉         | 7/80 [00:30<02:17,  1.88s/it]Convert to turbomind format:  10%|█         | 8/80 [00:30<01:35,  1.32s/it]Convert to turbomind format:  11%|█▏        | 9/80 [00:30<01:07,  1.05it/s]Convert to turbomind format:  12%|█▎        | 10/80 [00:30<00:49,  1.42it/s]Convert to turbomind format:  14%|█▍        | 11/80 [00:39<03:30,  3.05s/it]Convert to turbomind format:  15%|█▌        | 12/80 [00:39<02:27,  2.16s/it]Convert to turbomind format:  16%|█▋        | 13/80 [00:39<01:43,  1.55s/it]Convert to turbomind format:  18%|█▊        | 14/80 [00:39<01:13,  1.12s/it]Convert to turbomind format:  19%|█▉        | 15/80 [00:43<01:59,  1.83s/it]Convert to turbomind format:  20%|██        | 16/80 [00:43<01:24,  1.32s/it]Convert to turbomind format:  21%|██▏       | 17/80 [00:43<01:00,  1.04it/s]Convert to turbomind format:  22%|██▎       | 18/80 [00:43<00:44,  1.39it/s]Convert to turbomind format:  24%|██▍       | 19/80 [00:47<01:35,  1.57s/it]Convert to turbomind format:  25%|██▌       | 20/80 [00:47<01:08,  1.14s/it]Convert to turbomind format:  26%|██▋       | 21/80 [00:47<00:49,  1.20it/s]Convert to turbomind format:  28%|██▊       | 22/80 [00:47<00:36,  1.60it/s]Convert to turbomind format:  29%|██▉       | 23/80 [00:47<00:27,  2.07it/s]Convert to turbomind format:  30%|███       | 24/80 [00:56<02:42,  2.90s/it]Convert to turbomind format:  31%|███▏      | 25/80 [00:56<01:53,  2.07s/it]Convert to turbomind format:  32%|███▎      | 26/80 [00:56<01:20,  1.49s/it]Convert to turbomind format:  34%|███▍      | 27/80 [00:56<00:57,  1.08s/it]Convert to turbomind format:  35%|███▌      | 28/80 [01:00<01:34,  1.81s/it]Convert to turbomind format:  36%|███▋      | 29/80 [01:00<01:06,  1.31s/it]Convert to turbomind format:  38%|███▊      | 30/80 [01:00<00:47,  1.05it/s]Convert to turbomind format:  39%|███▉      | 31/80 [01:00<00:34,  1.41it/s]Convert to turbomind format:  40%|████      | 32/80 [01:03<01:13,  1.54s/it]Convert to turbomind format:  41%|████▏     | 33/80 [01:04<00:52,  1.12s/it]Convert to turbomind format:  42%|████▎     | 34/80 [01:04<00:37,  1.22it/s]Convert to turbomind format:  44%|████▍     | 35/80 [01:04<00:27,  1.63it/s]Convert to turbomind format:  45%|████▌     | 36/80 [01:04<00:20,  2.11it/s]Convert to turbomind format:  46%|████▋     | 37/80 [01:11<01:41,  2.37s/it]Convert to turbomind format:  48%|████▊     | 38/80 [01:11<01:11,  1.69s/it]Convert to turbomind format:  49%|████▉     | 39/80 [01:11<00:50,  1.22s/it]Convert to turbomind format:  50%|█████     | 40/80 [01:11<00:35,  1.11it/s]Convert to turbomind format:  51%|█████▏    | 41/80 [01:14<01:00,  1.55s/it]Convert to turbomind format:  52%|█████▎    | 42/80 [01:14<00:42,  1.12s/it]Convert to turbomind format:  54%|█████▍    | 43/80 [01:15<00:30,  1.21it/s]Convert to turbomind format:  55%|█████▌    | 44/80 [01:15<00:22,  1.62it/s]Convert to turbomind format:  56%|█████▋    | 45/80 [01:18<00:47,  1.36s/it]Convert to turbomind format:  57%|█████▊    | 46/80 [01:18<00:33,  1.01it/s]Convert to turbomind format:  59%|█████▉    | 47/80 [01:18<00:24,  1.37it/s]Convert to turbomind format:  60%|██████    | 48/80 [01:18<00:17,  1.80it/s]Convert to turbomind format:  61%|██████▏   | 49/80 [01:18<00:13,  2.32it/s]Convert to turbomind format:  62%|██████▎   | 50/80 [01:26<01:14,  2.47s/it]Convert to turbomind format:  64%|██████▍   | 51/80 [01:26<00:51,  1.77s/it]Convert to turbomind format:  65%|██████▌   | 52/80 [01:26<00:35,  1.28s/it]Convert to turbomind format:  66%|██████▋   | 53/80 [01:26<00:25,  1.07it/s]Convert to turbomind format:  68%|██████▊   | 54/80 [01:29<00:39,  1.53s/it]Convert to turbomind format:  69%|██████▉   | 55/80 [01:29<00:27,  1.11s/it]Convert to turbomind format:  70%|███████   | 56/80 [01:29<00:19,  1.23it/s]Convert to turbomind format:  71%|███████▏  | 57/80 [01:29<00:14,  1.63it/s]Convert to turbomind format:  72%|███████▎  | 58/80 [01:32<00:29,  1.34s/it]Convert to turbomind format:  74%|███████▍  | 59/80 [01:32<00:20,  1.02it/s]Convert to turbomind format:  75%|███████▌  | 60/80 [01:33<00:14,  1.38it/s]Convert to turbomind format:  76%|███████▋  | 61/80 [01:33<00:10,  1.83it/s]Convert to turbomind format:  78%|███████▊  | 62/80 [01:33<00:07,  2.34it/s]Convert to turbomind format:  79%|███████▉  | 63/80 [01:39<00:34,  2.01s/it]Convert to turbomind format:  80%|████████  | 64/80 [01:39<00:23,  1.44s/it]Convert to turbomind format:  81%|████████▏ | 65/80 [01:39<00:15,  1.05s/it]Convert to turbomind format:  82%|████████▎ | 66/80 [01:39<00:10,  1.29it/s]Convert to turbomind format:  84%|████████▍ | 67/80 [01:42<00:17,  1.38s/it]Convert to turbomind format:  85%|████████▌ | 68/80 [01:42<00:12,  1.01s/it]Convert to turbomind format:  86%|████████▋ | 69/80 [01:42<00:08,  1.35it/s]Convert to turbomind format:  88%|████████▊ | 70/80 [01:42<00:05,  1.78it/s]Convert to turbomind format:  89%|████████▉ | 71/80 [01:45<00:10,  1.18s/it]Convert to turbomind format:  90%|█████████ | 72/80 [01:45<00:06,  1.15it/s]Convert to turbomind format:  91%|█████████▏| 73/80 [01:45<00:04,  1.55it/s]Convert to turbomind format:  92%|█████████▎| 74/80 [01:45<00:02,  2.03it/s]Convert to turbomind format:  94%|█████████▍| 75/80 [01:45<00:01,  2.57it/s]Convert to turbomind format:  95%|█████████▌| 76/80 [01:50<00:07,  1.77s/it]Convert to turbomind format:  96%|█████████▋| 77/80 [01:50<00:03,  1.28s/it]Convert to turbomind format:  98%|█████████▊| 78/80 [01:51<00:01,  1.07it/s]Convert to turbomind format:  99%|█████████▉| 79/80 [01:51<00:00,  1.44it/s]Convert to turbomind format: 100%|██████████| 80/80 [01:55<00:00,  1.89s/it]                                                                            [WARNING] gemm_config.in is not found; using default GEMM algo
[WARNING] gemm_config.in is not found; using default GEMM algo
HINT:    Please open [93m[1mhttp://0.0.0.0:27182[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://0.0.0.0:27182[0m in a browser for detailed api usage!!!
HINT:    Please open [93m[1mhttp://0.0.0.0:27182[0m in a browser for detailed api usage!!!
INFO:     Started server process [768689]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:27182 (Press CTRL+C to quit)
INFO:     127.0.0.1:40646 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:42474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42466 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42428 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:42454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57820 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57752 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57798 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:57806 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:36:54,346 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:36:54,346 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:37:05,323 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:37:05,323 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:37:14,566 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:37:14,567 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
Failed to find a feasible kernel in the cache, will dispatch by heuristic.
2025-03-23 23:37:40,576 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
INFO:     127.0.0.1:37866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37912 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:37838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:38:36,969 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:38:45,532 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:38:45,532 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:38:45,533 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:38:54,144 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:38:54,144 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:39:05,031 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
INFO:     127.0.0.1:52506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52474 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52460 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52490 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52436 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52396 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52356 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:52380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:40:22,070 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:30,668 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:30,668 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:39,305 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:39,305 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:39,306 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:47,879 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:47,879 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:47,879 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:40:56,501 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:41:05,081 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:41:05,082 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
2025-03-23 23:41:05,082 - lmdeploy - [31mERROR[0m - async_engine.py:592 - [safe_run] exception caught: GeneratorExit 
INFO:     127.0.0.1:60518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55598 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55620 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:60518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:60530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55600 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:42:25,306 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:42:25,306 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=119.
INFO:     127.0.0.1:55298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:42:33,812 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:42:33,812 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=120.
INFO:     127.0.0.1:55306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:42:42,414 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:42:42,414 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=118.
INFO:     127.0.0.1:55282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:42:50,526 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:42:50,526 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=121.
INFO:     127.0.0.1:55286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:42:59,931 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:42:59,931 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=122.
INFO:     127.0.0.1:55298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:43:08,296 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:43:08,296 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=123.
INFO:     127.0.0.1:55306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:43:16,250 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:43:16,250 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=124.
INFO:     127.0.0.1:55282 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:43:26,048 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:43:26,048 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=125.
INFO:     127.0.0.1:55286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:43:34,224 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:43:34,224 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=126.
INFO:     127.0.0.1:55298 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:43:42,830 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:43:42,831 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=127.
INFO:     127.0.0.1:55306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:03,855 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:03,855 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=130.
INFO:     127.0.0.1:33446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:11,946 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:11,946 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=131.
INFO:     127.0.0.1:33462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:20,323 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:20,323 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=128.
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:28,430 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:28,430 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=129.
INFO:     127.0.0.1:33430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:37,077 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:37,077 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=132.
INFO:     127.0.0.1:33446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:45,767 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:45,768 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=133.
INFO:     127.0.0.1:33462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:44:54,048 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:44:54,049 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=134.
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:03,239 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:03,239 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=135.
INFO:     127.0.0.1:33430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:11,637 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:11,637 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=136.
INFO:     127.0.0.1:33446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:20,219 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:20,219 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=137.
INFO:     127.0.0.1:33462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:29,141 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:29,141 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=138.
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:37,816 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:37,816 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=139.
INFO:     127.0.0.1:33430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:46,454 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:46,455 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=140.
INFO:     127.0.0.1:33446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:45:56,528 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:45:56,528 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=141.
INFO:     127.0.0.1:33462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:46:05,221 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:46:05,221 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=142.
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:46:13,645 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:46:13,645 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=143.
INFO:     127.0.0.1:33430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:46:23,424 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:46:23,424 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=144.
INFO:     127.0.0.1:33446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:46:31,843 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:46:31,843 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=145.
INFO:     127.0.0.1:33462 - "POST /v1/chat/completions HTTP/1.1" 200 OK
2025-03-23 23:46:40,638 - lmdeploy - [31mERROR[0m - async_engine.py:690 - Truncate max_new_tokens to 128
2025-03-23 23:46:40,639 - lmdeploy - [31mERROR[0m - async_engine.py:692 - run out of tokens. session=146.
INFO:     127.0.0.1:33422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     Shutting down
INFO:     Waiting for background tasks to complete. (CTRL+C to force quit)
INFO:     Finished server process [768689]
ERROR:    Traceback (most recent call last):
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 700, in lifespan
    await receive()
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/uvicorn/lifespan/on.py", line 137, in receive
    return await self.receive_queue.get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError

ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/middleware/errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/middleware/cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/starlette/routing.py", line 73, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/fastapi/routing.py", line 301, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/fastapi/routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/lmdeploy/serve/openai/api_server.py", line 451, in chat_completions_v1
    async for res in result_generator:
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/lmdeploy/serve/async_engine.py", line 663, in generate
    prompt_input = await self._get_prompt_input(prompt,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/lmdeploy/serve/vl_async_engine.py", line 89, in _get_prompt_input
    results = await self.vl_encoder.async_infer(results)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/lmdeploy/vl/engine.py", line 61, in async_infer
    outputs = await future
              ^^^^^^^^^^^^
asyncio.exceptions.CancelledError
