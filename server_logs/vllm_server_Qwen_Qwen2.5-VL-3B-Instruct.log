INFO 03-19 18:34:36 __init__.py:207] Automatically detected platform cuda.
INFO 03-19 18:34:36 api_server.py:912] vLLM API server version 0.7.3
INFO 03-19 18:34:36 api_server.py:913] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-VL-3B-Instruct', config='', host=None, port=27182, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-VL-3B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=32768, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt={'image': 9}, mm_processor_kwargs={'images_kwargs.do_resize': True, 'images_kwargs.size.shortest_edge': 3136, 'images_kwargs.size.longest_edge': 250880}, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f5169146020>)
INFO 03-19 18:34:36 api_server.py:209] Started engine process with PID 2500168
INFO 03-19 18:34:44 __init__.py:207] Automatically detected platform cuda.
INFO 03-19 18:34:50 config.py:549] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.
INFO 03-19 18:34:50 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-19 18:35:02 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 03-19 18:35:02 config.py:1382] Defaulting to use mp for distributed inference
INFO 03-19 18:35:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='Qwen/Qwen2.5-VL-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-VL-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs={'images_kwargs.do_resize': True, 'images_kwargs.size.shortest_edge': 3136, 'images_kwargs.size.longest_edge': 250880}, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 03-19 18:35:04 multiproc_worker_utils.py:300] Reducing Torch parallelism from 4 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 03-19 18:35:04 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 03-19 18:35:05 cuda.py:229] Using Flash Attention backend.
INFO 03-19 18:35:12 __init__.py:207] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:12 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:13 cuda.py:229] Using Flash Attention backend.
INFO 03-19 18:35:14 utils.py:916] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:14 utils.py:916] Found nccl from library libnccl.so.2
INFO 03-19 18:35:14 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:14 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 03-19 18:35:14 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /nfshomes/sriramb/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:14 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /nfshomes/sriramb/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 03-19 18:35:14 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_4a857f0c'), local_subscribe_port=39409, remote_subscribe_port=None)
INFO 03-19 18:35:14 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-VL-3B-Instruct...
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:14 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-VL-3B-Instruct...
INFO 03-19 18:35:14 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:14 config.py:3054] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]
INFO 03-19 18:35:15 weight_utils.py:254] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:15 weight_utils.py:254] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.90it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.80it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.81it/s]

INFO 03-19 18:35:16 model_runner.py:1115] Loading model weights took 3.6665 GB
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:18 model_runner.py:1115] Loading model weights took 3.6665 GB
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
WARNING 03-19 18:35:18 model_runner.py:1288] Computed max_num_seqs (min(256, 32768 // 163840)) to be less than 1. Setting it to the minimum value of 1.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m WARNING 03-19 18:35:19 model_runner.py:1288] Computed max_num_seqs (min(256, 32768 // 163840)) to be less than 1. Setting it to the minimum value of 1.
Keyword argument `images_kwargs.do_resize` is not a valid argument for this processor and will be ignored.
Keyword argument `images_kwargs.size.shortest_edge` is not a valid argument for this processor and will be ignored.
Keyword argument `images_kwargs.size.longest_edge` is not a valid argument for this processor and will be ignored.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m Keyword argument `images_kwargs.do_resize` is not a valid argument for this processor and will be ignored.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m Keyword argument `images_kwargs.size.shortest_edge` is not a valid argument for this processor and will be ignored.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m Keyword argument `images_kwargs.size.longest_edge` is not a valid argument for this processor and will be ignored.
It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.
Token indices sequence length is longer than the specified maximum sequence length for this model (163840 > 131072). Running this sequence through the model will result in indexing errors
[1;36m(VllmWorkerProcess pid=2500255)[0;0m Token indices sequence length is longer than the specified maximum sequence length for this model (163840 > 131072). Running this sequence through the model will result in indexing errors
WARNING 03-19 18:35:30 profiling.py:192] The context length (32768) of the model is too short to hold the multi-modal embeddings in the worst case (163840 tokens in total, out of which {'image': 147456, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m WARNING 03-19 18:35:32 profiling.py:192] The context length (32768) of the model is too short to hold the multi-modal embeddings in the worst case (163840 tokens in total, out of which {'image': 147456, 'video': 16384} are reserved for multi-modal embeddings). This may cause certain multi-modal inputs to fail during inference, even when the input text is short. To avoid this, you should increase `max_model_len`, reduce `max_num_seqs`, and/or reduce `mm_counts`.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:39 worker.py:267] Memory profiling takes 21.34 seconds
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (15.72GiB) x gpu_memory_utilization (0.90) = 14.15GiB
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:39 worker.py:267] model weights take 3.67GiB; non_torch_memory takes 0.28GiB; PyTorch activation peak memory takes 1.64GiB; the rest of the memory reserved for KV Cache is 8.57GiB.
INFO 03-19 18:35:39 worker.py:267] Memory profiling takes 21.48 seconds
INFO 03-19 18:35:39 worker.py:267] the current vLLM instance can use total_gpu_memory (15.72GiB) x gpu_memory_utilization (0.90) = 14.15GiB
INFO 03-19 18:35:39 worker.py:267] model weights take 3.67GiB; non_torch_memory takes 0.29GiB; PyTorch activation peak memory takes 1.64GiB; the rest of the memory reserved for KV Cache is 8.55GiB.
INFO 03-19 18:35:40 executor_base.py:111] # cuda blocks: 31133, # CPU blocks: 14563
INFO 03-19 18:35:40 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 15.20x
INFO 03-19 18:35:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:35:42 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:18,  1.84it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:17,  1.91it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:16,  1.93it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:16,  1.94it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:15,  1.94it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:15,  1.93it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:14,  1.93it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:14,  1.92it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:13,  1.91it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:12,  1.93it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:12,  1.94it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:11,  1.94it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:11,  1.95it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:10,  1.95it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:07<00:10,  1.96it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:09,  1.96it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:08<00:09,  1.96it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:08,  1.95it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:09<00:08,  1.95it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:10<00:07,  1.94it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:10<00:07,  1.93it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:11<00:06,  1.93it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:11<00:06,  1.93it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:12<00:05,  1.92it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:12<00:05,  1.92it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:13<00:04,  1.92it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:13<00:04,  1.93it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:14<00:03,  1.94it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:14<00:03,  1.95it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:15<00:02,  1.95it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:16<00:02,  1.94it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:16<00:01,  1.94it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:17<00:01,  1.94it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:17<00:00,  1.94it/s][1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:36:00 custom_all_reduce.py:226] Registering 2555 cuda graph addresses
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.59it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.90it/s]
INFO 03-19 18:36:00 custom_all_reduce.py:226] Registering 2555 cuda graph addresses
[1;36m(VllmWorkerProcess pid=2500255)[0;0m INFO 03-19 18:36:00 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.40 GiB
INFO 03-19 18:36:00 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.40 GiB
INFO 03-19 18:36:00 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 42.39 seconds
INFO 03-19 18:36:01 api_server.py:958] Starting vLLM API server on http://0.0.0.0:27182
INFO 03-19 18:36:01 launcher.py:23] Available routes are:
INFO 03-19 18:36:01 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD
INFO 03-19 18:36:01 launcher.py:31] Route: /docs, Methods: GET, HEAD
INFO 03-19 18:36:01 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 03-19 18:36:01 launcher.py:31] Route: /redoc, Methods: GET, HEAD
INFO 03-19 18:36:01 launcher.py:31] Route: /health, Methods: GET
INFO 03-19 18:36:01 launcher.py:31] Route: /ping, Methods: POST, GET
INFO 03-19 18:36:01 launcher.py:31] Route: /tokenize, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /detokenize, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/models, Methods: GET
INFO 03-19 18:36:01 launcher.py:31] Route: /version, Methods: GET
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/chat/completions, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/completions, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/embeddings, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /pooling, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /score, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/score, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /rerank, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v1/rerank, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /v2/rerank, Methods: POST
INFO 03-19 18:36:01 launcher.py:31] Route: /invocations, Methods: POST
INFO:     Started server process [2500121]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 03-19 18:37:10 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 03-19 18:37:10 logger.py:39] Received request chatcmpl-8fc16bf1a5ad4473befd69dfa1f2345b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the trailer (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) trailer\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the air conditioner (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) air conditioner\n(B) desk\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) refrigerator\n(B) door\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the tray (highlighted by a blue box)?\n(A) refrigerator\n(B) tray\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) pedestrian\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the motorcycle (highlighted by a blue box)?\n(A) car\n(B) motorcycle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the motorcycle (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) motorcycle\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) door\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the desk (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) desk\n(B) chair\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:10 logger.py:39] Received request chatcmpl-6e9c6e8d57424787b0c4f4c442430dce: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the trailer (highlighted by a blue box)?\n(A) traffic cone\n(B) trailer\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the printer (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) printer\n(B) pillow\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the keyboard (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) keyboard\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the vase (highlighted by a red box) or the potted plant (highlighted by a blue box)?\n(A) vase\n(B) potted plant\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the barrier (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) barrier\n(B) car\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the painting (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) painting\n(B) sofa\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the sofa (highlighted by a red box) or the vase (highlighted by a blue box)?\n(A) sofa\n(B) vase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) car\n(B) truck\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:10 logger.py:39] Received request chatcmpl-a20d92b879454330b449a4806166c7ee: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) pedestrian\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the bin (highlighted by a blue box)?\n(A) lamp\n(B) bin\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the night stand (highlighted by a blue box)?\n(A) chair\n(B) night stand\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) books\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) truck\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) truck\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the dresser (highlighted by a red box) or the box (highlighted by a blue box)?\n(A) dresser\n(B) box\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) pillow\n(B) desk\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:11 logger.py:39] Received request chatcmpl-a9b2ce408e214e50ac34e386d14bde3c: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) chair\n(B) monitor\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bin (highlighted by a red box) or the printer (highlighted by a blue box)?\n(A) bin\n(B) printer\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) traffic cone\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the box (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) box\n(B) monitor\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the keyboard (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) keyboard\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the monitor (highlighted by a red box) or the clothes (highlighted by a blue box)?\n(A) monitor\n(B) clothes\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) door\n(B) lamp\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:14 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:14 engine.py:280] Added request chatcmpl-8fc16bf1a5ad4473befd69dfa1f2345b.
WARNING 03-19 18:37:15 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:15 engine.py:280] Added request chatcmpl-6e9c6e8d57424787b0c4f4c442430dce.
WARNING 03-19 18:37:15 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:15 engine.py:280] Added request chatcmpl-a20d92b879454330b449a4806166c7ee.
WARNING 03-19 18:37:15 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:15 engine.py:280] Added request chatcmpl-a9b2ce408e214e50ac34e386d14bde3c.
INFO 03-19 18:37:19 metrics.py:455] Avg prompt throughput: 714.3 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:51126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51144 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:37:26 logger.py:39] Received request chatcmpl-259a438d39b548e39ce7d1cf675ffd37: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the box (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) box\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the sofa (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) sofa\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the potted plant (highlighted by a blue box)?\n(A) lamp\n(B) potted plant\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the traffic cone (highlighted by a blue box)?\n(A) bus\n(B) traffic cone\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) table\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) door\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the monitor (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) monitor\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the dresser (highlighted by a red box) or the box (highlighted by a blue box)?\n(A) dresser\n(B) box\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:26 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:26 engine.py:280] Added request chatcmpl-259a438d39b548e39ce7d1cf675ffd37.
INFO 03-19 18:37:26 logger.py:39] Received request chatcmpl-d4a0374f37e14ca483861d61aea6c9ee: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) table\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the clock (highlighted by a red box) or the bottle (highlighted by a blue box)?\n(A) clock\n(B) bottle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) traffic cone\n(B) bicycle\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) television\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the toys (highlighted by a red box) or the closet (highlighted by a blue box)?\n(A) toys\n(B) closet\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the trailer (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) trailer\n(B) bus\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) table\n(B) sofa\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) table\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) television\n(B) sofa\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:26 logger.py:39] Received request chatcmpl-d91788462a034694b739164c00961864: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) books\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the bag (highlighted by a red box) or the remote (highlighted by a blue box)?\n(A) bag\n(B) remote\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the towel (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) towel\n(B) lamp\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) truck\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) pillow\n(B) lamp\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bin (highlighted by a red box) or the box (highlighted by a blue box)?\n(A) bin\n(B) box\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) car\n(B) bicycle\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) pedestrian\n(B) bicycle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:26 logger.py:39] Received request chatcmpl-6ca3b1b86488472ba8c1a8958ff3bc79: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) books\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) chair\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) table\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) truck\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the sofa (highlighted by a red box) or the picture (highlighted by a blue box)?\n(A) sofa\n(B) picture\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) refrigerator\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the dresser (highlighted by a red box) or the potted plant (highlighted by a blue box)?\n(A) dresser\n(B) potted plant\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) truck\n(B) bus\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the blinds (highlighted by a blue box)?\n(A) lamp\n(B) blinds\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:27 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:27 engine.py:280] Added request chatcmpl-d4a0374f37e14ca483861d61aea6c9ee.
WARNING 03-19 18:37:27 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:27 engine.py:280] Added request chatcmpl-d91788462a034694b739164c00961864.
WARNING 03-19 18:37:27 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:27 engine.py:280] Added request chatcmpl-6ca3b1b86488472ba8c1a8958ff3bc79.
INFO 03-19 18:37:29 metrics.py:455] Avg prompt throughput: 314.8 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:33834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33866 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:33846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:37:39 logger.py:39] Received request chatcmpl-d8ff10254ba94b71bf2d684ade7269f8: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the bicycle (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) bicycle\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) car\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) chair\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the night stand (highlighted by a blue box)?\n(A) lamp\n(B) night stand\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) refrigerator\n(B) door\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) truck\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) table\n(B) television\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:39 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:39 engine.py:280] Added request chatcmpl-d8ff10254ba94b71bf2d684ade7269f8.
INFO 03-19 18:37:39 logger.py:39] Received request chatcmpl-d5138bc0e5af48ba866e936905dcc912: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) lamp\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) truck\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) books\n(B) monitor\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the trailer (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) trailer\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the shelves (highlighted by a blue box)?\n(A) books\n(B) shelves\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the fireplace (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) fireplace\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) bookcase\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the sink (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) sink\n(B) pillow\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:39 logger.py:39] Received request chatcmpl-e4079b1b80064358bfbf3921dc4ab8aa: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) pillow\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) chair\n(B) monitor\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) chair\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the curtain (highlighted by a red box) or the box (highlighted by a blue box)?\n(A) curtain\n(B) box\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the barrier (highlighted by a red box) or the trailer (highlighted by a blue box)?\n(A) barrier\n(B) trailer\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) bus\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the trailer (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) trailer\n(B) car\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the vase (highlighted by a red box) or the potted plant (highlighted by a blue box)?\n(A) vase\n(B) potted plant\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) books\n(B) bookcase\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:39 logger.py:39] Received request chatcmpl-28b32467d13d47f2b0827c48b5a8d1ed: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) car\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) pedestrian\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) car\n(B) bicycle\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) television\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the trailer (highlighted by a blue box)?\n(A) traffic cone\n(B) trailer\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the motorcycle (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) motorcycle\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the picture (highlighted by a blue box)?\n(A) table\n(B) picture\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the monitor (highlighted by a red box) or the clothes (highlighted by a blue box)?\n(A) monitor\n(B) clothes\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) door\n(B) pillow\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:40 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:40 engine.py:280] Added request chatcmpl-d5138bc0e5af48ba866e936905dcc912.
WARNING 03-19 18:37:40 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:40 engine.py:280] Added request chatcmpl-e4079b1b80064358bfbf3921dc4ab8aa.
WARNING 03-19 18:37:40 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:40 engine.py:280] Added request chatcmpl-28b32467d13d47f2b0827c48b5a8d1ed.
INFO 03-19 18:37:42 metrics.py:455] Avg prompt throughput: 996.2 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:45172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:45184 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:37:46 logger.py:39] Received request chatcmpl-020fe2bdf5dd46fc8a916945322a340f: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) books\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the barrier (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) barrier\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) table\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the shelves (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) shelves\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) chair\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the refrigerator (highlighted by a blue box)?\n(A) lamp\n(B) refrigerator\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:47 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:47 engine.py:280] Added request chatcmpl-020fe2bdf5dd46fc8a916945322a340f.
INFO 03-19 18:37:47 metrics.py:455] Avg prompt throughput: 2320.0 tokens/s, Avg generation throughput: 2.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:45172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:37:51 logger.py:39] Received request chatcmpl-8e25eab0a5b5485dbe750c8aa5e47898: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the bin (highlighted by a red box) or the microwave (highlighted by a blue box)?\n(A) bin\n(B) microwave\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) television\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) chair\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) television\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) truck\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) lamp\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) books\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the person (highlighted by a red box) or the mirror (highlighted by a blue box)?\n(A) person\n(B) mirror\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) books\n(B) lamp\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:52 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:52 engine.py:280] Added request chatcmpl-8e25eab0a5b5485dbe750c8aa5e47898.
INFO 03-19 18:37:52 logger.py:39] Received request chatcmpl-b03855b651884956809db3d4254f50f6: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) pillow\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) television\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the tissues (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) tissues\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) television\n(B) sofa\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) chair\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) bookcase\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the barrier (highlighted by a blue box)?\n(A) bus\n(B) barrier\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) refrigerator\n(B) door\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:52 logger.py:39] Received request chatcmpl-6ffb68307b3b4ff3a8d41115582144f9: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) pedestrian\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the machine (highlighted by a red box) or the tissues (highlighted by a blue box)?\n(A) machine\n(B) tissues\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) traffic cone\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the stationery (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) stationery\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the box (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) box\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) traffic cone\n(B) pedestrian\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the board (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) board\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) chair\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:37:52 logger.py:39] Received request chatcmpl-97499775ee1e40a2b7ce214bd476df01: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the towel (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) towel\n(B) pillow\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the barrier (highlighted by a blue box)?\n(A) bus\n(B) barrier\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the barrier (highlighted by a blue box)?\n(A) bus\n(B) barrier\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) pedestrian\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) television\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the sofa (highlighted by a blue box)?\n(A) table\n(B) sofa\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) door\n(B) pillow\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the desk (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) desk\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:37:53 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:53 engine.py:280] Added request chatcmpl-b03855b651884956809db3d4254f50f6.
WARNING 03-19 18:37:53 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:53 engine.py:280] Added request chatcmpl-6ffb68307b3b4ff3a8d41115582144f9.
WARNING 03-19 18:37:53 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:37:53 engine.py:280] Added request chatcmpl-97499775ee1e40a2b7ce214bd476df01.
INFO 03-19 18:37:55 metrics.py:455] Avg prompt throughput: 449.7 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:45172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:48452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:48450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:48456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:38:04 logger.py:39] Received request chatcmpl-d528b9c517c24bc8a66eb34ed80f5bef: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the shelves (highlighted by a blue box)?\n(A) lamp\n(B) shelves\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) lamp\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) pedestrian\n(B) car\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) truck\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the bin (highlighted by a red box) or the microwave (highlighted by a blue box)?\n(A) bin\n(B) microwave\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) bookcase\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) chair\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) chair\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the shelves (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) shelves\n(B) television\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:38:04 logger.py:39] Received request chatcmpl-96ee9be320b149b68436b9cdddc911c1: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) pillow\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the bin (highlighted by a blue box)?\n(A) chair\n(B) bin\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) television\n(B) table\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) books\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) car\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the pillow (highlighted by a blue box)?\n(A) table\n(B) pillow\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the stationery (highlighted by a red box) or the bag (highlighted by a blue box)?\n(A) stationery\n(B) bag\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) bus\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the television (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) television\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:38:04 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:04 engine.py:280] Added request chatcmpl-d528b9c517c24bc8a66eb34ed80f5bef.
WARNING 03-19 18:38:04 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:04 engine.py:280] Added request chatcmpl-96ee9be320b149b68436b9cdddc911c1.
INFO 03-19 18:38:04 logger.py:39] Received request chatcmpl-12c80eab2ce346569f613dd244287aca: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the shelves (highlighted by a blue box)?\n(A) table\n(B) shelves\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) bus\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the trailer (highlighted by a blue box)?\n(A) bus\n(B) trailer\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the barrier (highlighted by a blue box)?\n(A) truck\n(B) barrier\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) pedestrian\n(B) car\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) truck\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the rack (highlighted by a blue box)?\n(A) books\n(B) rack\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the refrigerator (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) refrigerator\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) table\n(B) door\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:38:05 logger.py:39] Received request chatcmpl-0f8e4ed7f9ca49a8a116c0dcf3e423ab: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the traffic cone (highlighted by a red box) or the trailer (highlighted by a blue box)?\n(A) traffic cone\n(B) trailer\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) door\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) car\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the traffic cone (highlighted by a blue box)?\n(A) pedestrian\n(B) traffic cone\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the television (highlighted by a blue box)?\n(A) books\n(B) television\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) lamp\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the tissues (highlighted by a red box) or the bin (highlighted by a blue box)?\n(A) tissues\n(B) bin\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the traffic cone (highlighted by a blue box)?\n(A) bus\n(B) traffic cone\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) door\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:38:06 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:06 engine.py:280] Added request chatcmpl-12c80eab2ce346569f613dd244287aca.
WARNING 03-19 18:38:07 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:07 engine.py:280] Added request chatcmpl-0f8e4ed7f9ca49a8a116c0dcf3e423ab.
INFO 03-19 18:38:08 metrics.py:455] Avg prompt throughput: 1229.3 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:51780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51808 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:51814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:38:16 logger.py:39] Received request chatcmpl-e792a3fcf4f54e708fb3cc9af0075099: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the picture (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) picture\n(B) door\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) table\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) pedestrian\n(B) car\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the motorcycle (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) motorcycle\n(B) bicycle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) books\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the desk (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) desk\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) car\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the chair (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) chair\n(B) bookcase\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:38:16 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:16 engine.py:280] Added request chatcmpl-e792a3fcf4f54e708fb3cc9af0075099.
INFO 03-19 18:38:16 logger.py:39] Received request chatcmpl-70b8fa9020724152a140aa4408491d40: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) table\n(B) door\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the sink (highlighted by a red box) or the bottle (highlighted by a blue box)?\n(A) sink\n(B) bottle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the car (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) car\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) table\n(B) books\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the truck (highlighted by a blue box)?\n(A) pedestrian\n(B) truck\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the bottle (highlighted by a red box) or the remote (highlighted by a blue box)?\n(A) bottle\n(B) remote\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the pedestrian (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) pedestrian\n(B) bicycle\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the bus (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) bus\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:38:16 logger.py:39] Received request chatcmpl-fed85a86c9e740acb7b72eab4043347b: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the pillow (highlighted by a red box) or the desk (highlighted by a blue box)?\n(A) pillow\n(B) desk\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the door (highlighted by a red box) or the lamp (highlighted by a blue box)?\n(A) door\n(B) lamp\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the tissues (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) tissues\n(B) chair\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the bicycle (highlighted by a red box) or the car (highlighted by a blue box)?\n(A) bicycle\n(B) car\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the bicycle (highlighted by a red box) or the bus (highlighted by a blue box)?\n(A) bicycle\n(B) bus\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the towel (highlighted by a blue box)?\n(A) lamp\n(B) towel\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the books (highlighted by a red box) or the shelves (highlighted by a blue box)?\n(A) books\n(B) shelves\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) table\n(B) monitor\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the books (highlighted by a blue box)?\n(A) bookcase\n(B) books\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 03-19 18:38:16 logger.py:39] Received request chatcmpl-fe309607a30a44c8821dab5477d1273d: prompt: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>\nExample 0:\nWhich object is closer to the camera taking this photo, the bin (highlighted by a red box) or the monitor (highlighted by a blue box)?\n(A) bin\n(B) monitor\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 1:\nWhich object is closer to the camera taking this photo, the bag (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) bag\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 2:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the door (highlighted by a blue box)?\n(A) lamp\n(B) door\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 3:\nWhich object is closer to the camera taking this photo, the lamp (highlighted by a red box) or the chair (highlighted by a blue box)?\n(A) lamp\n(B) chair\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 4:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 5:\nWhich object is closer to the camera taking this photo, the truck (highlighted by a red box) or the pedestrian (highlighted by a blue box)?\n(A) truck\n(B) pedestrian\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 6:\nWhich object is closer to the camera taking this photo, the table (highlighted by a red box) or the bookcase (highlighted by a blue box)?\n(A) table\n(B) bookcase\n The answer is (A)\n<|vision_start|><|image_pad|><|vision_end|>\nExample 7:\nWhich object is closer to the camera taking this photo, the motorcycle (highlighted by a red box) or the bicycle (highlighted by a blue box)?\n(A) motorcycle\n(B) bicycle\n The answer is (B)\n<|vision_start|><|image_pad|><|vision_end|>\nQuestion:\nWhich object is closer to the camera taking this photo, the bookcase (highlighted by a red box) or the table (highlighted by a blue box)?\n(A) bookcase\n(B) table\nEnd your answer with either (A) or (B).\nThink step by step and carefully reason through the question before giving the answer.<|im_end|>\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
WARNING 03-19 18:38:17 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:17 engine.py:280] Added request chatcmpl-70b8fa9020724152a140aa4408491d40.
WARNING 03-19 18:38:17 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:17 engine.py:280] Added request chatcmpl-fed85a86c9e740acb7b72eab4043347b.
WARNING 03-19 18:38:18 utils.py:1446] The following intended overrides are not keyword-only args and and will be dropped: {'images_kwargs.size.longest_edge', 'images_kwargs.size.shortest_edge', 'images_kwargs.do_resize'}
INFO 03-19 18:38:18 engine.py:280] Added request chatcmpl-fe309607a30a44c8821dab5477d1273d.
INFO 03-19 18:38:19 metrics.py:455] Avg prompt throughput: 842.7 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%.
INFO:     127.0.0.1:39068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 03-19 18:38:30 metrics.py:455] Avg prompt throughput: 901.9 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 03-19 18:38:40 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
